"""
Core Logic for Retrieval-Augmented Generation (RAG) System
=========================================================
This module implements the core pipeline for the RAG system, including embedding generation,
retrieval from the vector database, and answer synthesis using a large language model (LLM).

Key Responsibilities:
- Encapsulates the main async RAG pipeline logic, decoupled from web or CLI frameworks.
- Handles embedding, retrieval, prompt construction, and streaming LLM responses.
- Provides robust error handling for API quota, safety filters, and unexpected failures.

Usage:
Call `generate_answer_stream` from the API or CLI layer to obtain a streaming answer for a user query.
"""

import logging
import asyncio
from typing import AsyncGenerator
import chromadb
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions
from google.generativeai.types import generation_types
from app.config import Settings

default_prompt_instructions = """
You are a knowledgeable and respectful Islamic assistant. Your purpose is to answer the user's question with accuracy, based strictly and exclusively on the context documents provided below.
**Instructions:**
1.  Carefully analyze the documents to answer the user's question.
2.  If the documents do not contain the necessary information, you MUST state: "Based on the provided documents, I cannot answer this question."
3.  After answering, cite the source URL(s) on a new line. Format: `Source(s): ...`.
"""

async def generate_answer_stream(
    query: str, 
    settings: Settings,
    collection: chromadb.Collection,
    embedding_model: SentenceTransformer,
    llm: genai.GenerativeModel,
) -> AsyncGenerator[str, None]:
    """
    Main RAG Pipeline (Async)
    ------------------------
    Given a user query, retrieves relevant documents, constructs a prompt, and streams the LLM's answer.
    Handles all error cases gracefully and yields the answer in a streaming fashion.

    Args:
        query (str): The user's question.
        settings (Settings): Application configuration.
        collection (chromadb.Collection): The vector database collection to search.
        embedding_model (SentenceTransformer): The embedding model for query encoding.
        llm (genai.GenerativeModel): The LLM for answer generation.

    Yields:
        str: Chunks of the answer as they are generated by the LLM.
    """
    try:
        # Run synchronous, CPU-bound code in a thread pool to avoid blocking
        query_embedding = await asyncio.to_thread(embedding_model.encode, query, normalize_embeddings=True)
        results = await asyncio.to_thread(collection.query, query_embeddings=[query_embedding.tolist()], n_results=settings.N_RESULTS_RETRIEVAL)

        context_docs = results['documents'][0]
        sources = sorted(list(set(meta.get('source', 'Unknown') for meta in results['metadatas'][0])))

        prompt = f"""<|system|>\n{default_prompt_instructions}</s><|user|>\n<documents>\n{"\n\n---\n\n".join(context_docs)}\n</documents>\nQuestion: \"{query}\"\n</s><|assistant|>"""

        stream = await llm.generate_content_async(prompt, stream=True)
        content_yielded = False
        async for chunk in stream:
            if chunk.text:
                content_yielded = True
                yield chunk.text
        if not content_yielded:
            yield "The model returned an empty response."

    except google_exceptions.ResourceExhausted as e:
        logging.warning(f"Google API Quota Exhausted. Details: {e.message}")
        yield "**The AI service is temporarily unavailable due to high demand (API Quota Exceeded). Please try again later.**"
    except generation_types.StopCandidateException as e:
        logging.warning(f"Response blocked by safety filters: {e}")
        yield "**The response was blocked by the model's safety filters.**"
    except Exception as e:
        logging.error(f"An unexpected error occurred in the RAG pipeline: {e}", exc_info=True)
        yield "**An unexpected error occurred. Please contact support.**"